{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ea3dc3",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "import collections\n",
    "import shutil\n",
    "import ssl \n",
    "import copy\n",
    "import math\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "import albumentations as A\n",
    "import cv2 as cv \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset,DataLoader,random_split,Subset\n",
    "from torchvision import datasets,models,transforms\n",
    "ssl._create_default_https_context = ssl._create_unverified_context \n",
    "import PIL          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc157c",
   "metadata": {},
   "source": [
    "**Background**:Rice (Oryza sativa) is one of the staple foods worldwide. Paddy, the raw grain before removal of husk, is cultivated in tropical climates, mainly in Asian countries. Paddy cultivation requires consistent supervision because several diseases and pests might affect the paddy crops, leading to up to 70% yield loss. Expert supervision is usually necessary to mitigate these diseases and prevent crop loss. With the limited availability of crop protection experts, manual disease diagnosis is tedious and expensive. Thus, it is increasingly important to automate the disease identification process by leveraging computer vision-based techniques that achieved promising results in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef7235",
   "metadata": {},
   "source": [
    "**Objective**:Develop a machine or deep learning-based model to classify the given paddy leaf images accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d7517",
   "metadata": {},
   "source": [
    "**Type**:Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7ddba",
   "metadata": {},
   "source": [
    "**Scale**:\n",
    "training dataset of 10,407 (75%) labeled images across ten classes (nine disease categories and normal leaf) and additional metadata for each image, such as the paddy variety and age.\n",
    "\n",
    "test dataset of 3,469 (25%) images into one of the nine disease categories or a normal leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe14597",
   "metadata": {},
   "source": [
    "**Evaluation**:Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742e30e1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# transforms transform \n",
    "path = Path('/Users/wangshuo/Library/Mobile Documents/com~apple~CloudDocs/data/paddy-disease-classification/') \n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "class PaddyDataset(Dataset):\n",
    "    def __init__(self,dataset,transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "83689ee9",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# albumentations transform\n",
    "path = Path('/Users/wangshuo/Library/Mobile Documents/com~apple~CloudDocs/data/paddy-disease-classification/') \n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(),\n",
    "    A.VerticalFlip(),\n",
    "    A.RandomRotate90(),\n",
    "    A.ImageCompression(),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2,scale_limit=0.2,rotate_limit=45,border_mode=0),\n",
    "    A.Resize(height=224,width=224),\n",
    "    A.CoarseDropout(max_height=int(224*0.4),max_width=int(224*0.4),max_holes=1,p=0.75),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(height=224,width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2() \n",
    "])\n",
    "\n",
    "Test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "class PaddyDataset(Dataset):\n",
    "    def __init__(self,dataset,transform):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        img = self.transform(image=np.array(img))['image']\n",
    "        return img, label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1830792",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data \n",
    "data = datasets.ImageFolder(path/'train_images')\n",
    "\n",
    "skf = StratifiedKFold(5, shuffle=True, random_state=7).split(data,data.targets)\n",
    "\n",
    "train_loader , test_loader = [], []\n",
    "\n",
    "for train__idx, test_idx in skf:\n",
    "    train_data = Subset(data,train__idx)\n",
    "    test_data = Subset(data,test_idx)\n",
    "    \n",
    "    train_data = PaddyDataset(train_data,transform=train_transform)\n",
    "    test_data = PaddyDataset(test_data,transform=test_transform)\n",
    "    \n",
    "    train_loader.append(DataLoader(train_data,batch_size=16,shuffle=True))\n",
    "    test_loader.append(DataLoader(test_data,batch_size=16,shuffle=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f33a0095",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total\n",
    "\n",
    "def train(model, train_data, test_data, num_epochs, optimizer, criterion,scheduler):\n",
    "    \n",
    "    best_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    prev_time = datetime.now()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        for im, label in train_data:\n",
    "            # forward\n",
    "            im = im.to('mps')\n",
    "            label = label.to('mps')\n",
    "            \n",
    "            output = model(im)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(output, label)\n",
    "            \n",
    "        scheduler.step()\n",
    "        cur_time = datetime.now()\n",
    "        \n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s) \n",
    "        \n",
    "        if test_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            model = model.eval()\n",
    "            \n",
    "            for im, label in test_data:\n",
    "                im = im.to('mps')\n",
    "                label = label.to('mps')\n",
    "                output = model(im)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(output, label)\n",
    "                epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(test_data),\n",
    "                   valid_acc / len(test_data)))\n",
    "            valid_acc = valid_acc /  len(test_data)\n",
    "            if valid_acc > best_acc:\n",
    "                best_acc = valid_acc\n",
    "                best_wts = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n",
    "                         (epoch, train_loss / len(train_data),\n",
    "                          train_acc / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str)\n",
    "    model.load_state_dict(best_wts)\n",
    "    return best_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b11ad328",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# train_new\n",
    "def train_new(model, train_data, test_data, num_epochs, optimizer, criterion,scheduler):\n",
    "    \n",
    "    best_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    prev_time = datetime.now()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # tain model\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        model = model.train()\n",
    "        \n",
    "        for im, label in train_data:\n",
    "            \n",
    "            im = im.to('mps:0')\n",
    "            label = label.to('mps:0')\n",
    "            # zero  gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            output = model(im)\n",
    "            # get loss\n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            # accuracy\n",
    "            _, pred_label = output.max(1)\n",
    "            num_correct = (pred_label == label).sum().item()    \n",
    "            train_acc += (num_correct / output.shape[0])\n",
    "        # update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        cur_time = datetime.now()\n",
    "        # time spent\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s) \n",
    "        \n",
    "        # accuracy on test data\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        \n",
    "        model = model.eval()\n",
    "\n",
    "        for im, label in test_data:\n",
    "            \n",
    "            im = im.to('mps:0')\n",
    "            label = label.to('mps:0')\n",
    "            \n",
    "            output = model(im)       \n",
    "            loss = criterion(output, label) \n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            _, pred_label = output.max(1)\n",
    "            num_correct = (pred_label == label).sum().item()    \n",
    "            valid_acc += (num_correct / output.shape[0])\n",
    "\n",
    "        epoch_str = (\n",
    "            \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "            % (epoch, train_loss / len(train_data),\n",
    "               train_acc / len(train_data), valid_loss / len(test_data),\n",
    "               valid_acc / len(test_data)))\n",
    "        # get the best parameters    \n",
    "        valid_acc = valid_acc /  len(test_data)\n",
    "        \n",
    "        if valid_acc > best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        prev_time = cur_time\n",
    "        \n",
    "        print(epoch_str + time_str)\n",
    "        \n",
    "    model.load_state_dict(best_wts)\n",
    "    \n",
    "    return best_acc, model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc2c0d5",
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 1.269723, Train Acc: 0.564851, Valid Loss: 0.875445, Valid Acc: 0.735687, Time 00:04:20\n",
      "Epoch 1. Train Loss: 0.728675, Train Acc: 0.758253, Valid Loss: 0.450567, Valid Acc: 0.851622, Time 00:04:30\n",
      "Epoch 2. Train Loss: 0.563484, Train Acc: 0.809501, Valid Loss: 0.362977, Valid Acc: 0.877863, Time 00:04:31\n",
      "Epoch 3. Train Loss: 0.444500, Train Acc: 0.852207, Valid Loss: 0.312301, Valid Acc: 0.901240, Time 00:04:33\n",
      "Epoch 4. Train Loss: 0.372592, Train Acc: 0.872073, Valid Loss: 0.302947, Valid Acc: 0.905057, Time 00:04:51\n",
      "Epoch 5. Train Loss: 0.299837, Train Acc: 0.899328, Valid Loss: 0.247030, Valid Acc: 0.932252, Time 00:05:09\n",
      "Epoch 6. Train Loss: 0.261324, Train Acc: 0.914347, Valid Loss: 0.220034, Valid Acc: 0.939885, Time 00:05:08\n",
      "Epoch 7. Train Loss: 0.229877, Train Acc: 0.924904, Valid Loss: 0.232943, Valid Acc: 0.936069, Time 00:05:09\n",
      "Epoch 8. Train Loss: 0.208341, Train Acc: 0.934141, Valid Loss: 0.168827, Valid Acc: 0.947519, Time 00:05:08\n",
      "Epoch 9. Train Loss: 0.194307, Train Acc: 0.933517, Valid Loss: 0.192485, Valid Acc: 0.947042, Time 00:05:11\n",
      "Epoch 10. Train Loss: 0.171830, Train Acc: 0.942394, Valid Loss: 0.183063, Valid Acc: 0.956107, Time 00:05:09\n",
      "Epoch 11. Train Loss: 0.110936, Train Acc: 0.963772, Valid Loss: 0.128586, Valid Acc: 0.968511, Time 00:05:10\n",
      "Epoch 12. Train Loss: 0.090685, Train Acc: 0.971785, Valid Loss: 0.108718, Valid Acc: 0.972805, Time 00:05:10\n",
      "Epoch 13. Train Loss: 0.082598, Train Acc: 0.972745, Valid Loss: 0.120987, Valid Acc: 0.969466, Time 00:05:09\n",
      "Epoch 14. Train Loss: 0.076908, Train Acc: 0.973249, Valid Loss: 0.112397, Valid Acc: 0.974237, Time 00:05:09\n",
      "Epoch 15. Train Loss: 0.074108, Train Acc: 0.973608, Valid Loss: 0.103521, Valid Acc: 0.975668, Time 00:05:10\n",
      "Epoch 16. Train Loss: 0.069294, Train Acc: 0.975528, Valid Loss: 0.098887, Valid Acc: 0.975668, Time 00:05:09\n",
      "Epoch 17. Train Loss: 0.064774, Train Acc: 0.978383, Valid Loss: 0.107130, Valid Acc: 0.976622, Time 00:05:09\n",
      "Epoch 18. Train Loss: 0.067776, Train Acc: 0.976008, Valid Loss: 0.101323, Valid Acc: 0.974237, Time 00:05:08\n",
      "Epoch 19. Train Loss: 0.067667, Train Acc: 0.978287, Valid Loss: 0.108844, Valid Acc: 0.974237, Time 00:05:09\n",
      "----------\n",
      "Epoch 0. Train Loss: 1.287537, Train Acc: 0.559021, Valid Loss: 1.070189, Valid Acc: 0.683206, Time 00:04:51\n",
      "Epoch 1. Train Loss: 0.735300, Train Acc: 0.752015, Valid Loss: 0.508546, Valid Acc: 0.838740, Time 00:05:13\n",
      "Epoch 2. Train Loss: 0.533079, Train Acc: 0.822073, Valid Loss: 0.429829, Valid Acc: 0.869752, Time 00:05:13\n",
      "Epoch 3. Train Loss: 0.424571, Train Acc: 0.857318, Valid Loss: 0.273572, Valid Acc: 0.917939, Time 00:05:14\n",
      "Epoch 4. Train Loss: 0.364754, Train Acc: 0.883589, Valid Loss: 0.320650, Valid Acc: 0.906966, Time 00:05:12\n",
      "Epoch 5. Train Loss: 0.308916, Train Acc: 0.898728, Valid Loss: 0.258220, Valid Acc: 0.928912, Time 00:05:10\n",
      "Epoch 6. Train Loss: 0.264099, Train Acc: 0.911564, Valid Loss: 0.235543, Valid Acc: 0.927004, Time 00:05:09\n",
      "Epoch 7. Train Loss: 0.231121, Train Acc: 0.924904, Valid Loss: 0.177751, Valid Acc: 0.952290, Time 00:05:10\n",
      "Epoch 8. Train Loss: 0.203607, Train Acc: 0.933181, Valid Loss: 0.179507, Valid Acc: 0.949905, Time 00:05:09\n",
      "Epoch 9. Train Loss: 0.188873, Train Acc: 0.932198, Valid Loss: 0.172790, Valid Acc: 0.952767, Time 00:05:08\n",
      "Epoch 10. Train Loss: 0.175448, Train Acc: 0.940019, Valid Loss: 0.171253, Valid Acc: 0.951336, Time 00:05:09\n",
      "Epoch 11. Train Loss: 0.107043, Train Acc: 0.966987, Valid Loss: 0.115192, Valid Acc: 0.966126, Time 00:05:09\n",
      "Epoch 12. Train Loss: 0.088073, Train Acc: 0.971569, Valid Loss: 0.110850, Valid Acc: 0.969943, Time 00:05:10\n",
      "Epoch 13. Train Loss: 0.077941, Train Acc: 0.973488, Valid Loss: 0.106369, Valid Acc: 0.968034, Time 00:05:09\n",
      "Epoch 14. Train Loss: 0.075993, Train Acc: 0.975528, Valid Loss: 0.106441, Valid Acc: 0.970420, Time 00:05:08\n",
      "Epoch 15. Train Loss: 0.068125, Train Acc: 0.975648, Valid Loss: 0.104980, Valid Acc: 0.968511, Time 00:05:10\n",
      "Epoch 16. Train Loss: 0.062909, Train Acc: 0.981166, Valid Loss: 0.102672, Valid Acc: 0.970420, Time 00:05:08\n",
      "Epoch 17. Train Loss: 0.062478, Train Acc: 0.979726, Valid Loss: 0.101709, Valid Acc: 0.968511, Time 00:05:08\n",
      "Epoch 18. Train Loss: 0.064783, Train Acc: 0.978887, Valid Loss: 0.102354, Valid Acc: 0.970897, Time 00:05:08\n",
      "Epoch 19. Train Loss: 0.067286, Train Acc: 0.979679, Valid Loss: 0.104026, Valid Acc: 0.968989, Time 00:05:08\n",
      "----------\n",
      "Epoch 0. Train Loss: 1.289906, Train Acc: 0.561980, Valid Loss: 0.716963, Valid Acc: 0.750000, Time 00:04:53\n",
      "Epoch 1. Train Loss: 0.751142, Train Acc: 0.752439, Valid Loss: 0.541859, Valid Acc: 0.820611, Time 00:05:08\n",
      "Epoch 2. Train Loss: 0.556327, Train Acc: 0.812980, Valid Loss: 0.369296, Valid Acc: 0.878817, Time 00:05:11\n",
      "Epoch 3. Train Loss: 0.443467, Train Acc: 0.847929, Valid Loss: 0.326003, Valid Acc: 0.909351, Time 00:05:09\n",
      "Epoch 4. Train Loss: 0.377395, Train Acc: 0.873401, Valid Loss: 0.211439, Valid Acc: 0.932729, Time 00:05:09\n",
      "Epoch 5. Train Loss: 0.318469, Train Acc: 0.894434, Valid Loss: 0.236664, Valid Acc: 0.932252, Time 00:05:10\n",
      "Epoch 6. Train Loss: 0.275935, Train Acc: 0.909189, Valid Loss: 0.224030, Valid Acc: 0.937023, Time 00:05:10\n",
      "Epoch 7. Train Loss: 0.249420, Train Acc: 0.912508, Valid Loss: 0.227090, Valid Acc: 0.941317, Time 00:05:09\n",
      "Epoch 8. Train Loss: 0.218545, Train Acc: 0.927903, Valid Loss: 0.163266, Valid Acc: 0.955153, Time 00:05:09\n",
      "Epoch 9. Train Loss: 0.188111, Train Acc: 0.938020, Valid Loss: 0.194288, Valid Acc: 0.948473, Time 00:05:09\n",
      "Epoch 10. Train Loss: 0.170830, Train Acc: 0.942418, Valid Loss: 0.216523, Valid Acc: 0.945611, Time 00:05:09\n",
      "Epoch 11. Train Loss: 0.118296, Train Acc: 0.961292, Valid Loss: 0.115405, Valid Acc: 0.972805, Time 00:05:09\n",
      "Epoch 12. Train Loss: 0.082144, Train Acc: 0.972769, Valid Loss: 0.108924, Valid Acc: 0.974237, Time 00:05:10\n",
      "Epoch 13. Train Loss: 0.081147, Train Acc: 0.972769, Valid Loss: 0.108256, Valid Acc: 0.973760, Time 00:05:10\n",
      "Epoch 14. Train Loss: 0.074282, Train Acc: 0.977087, Valid Loss: 0.109646, Valid Acc: 0.975668, Time 00:05:10\n",
      "Epoch 15. Train Loss: 0.070989, Train Acc: 0.977567, Valid Loss: 0.106118, Valid Acc: 0.978531, Time 00:05:11\n",
      "Epoch 16. Train Loss: 0.067352, Train Acc: 0.978087, Valid Loss: 0.105090, Valid Acc: 0.977576, Time 00:05:10\n",
      "Epoch 17. Train Loss: 0.061929, Train Acc: 0.979287, Valid Loss: 0.104632, Valid Acc: 0.977099, Time 00:05:09\n",
      "Epoch 18. Train Loss: 0.063715, Train Acc: 0.979367, Valid Loss: 0.108100, Valid Acc: 0.976145, Time 00:05:09\n",
      "Epoch 19. Train Loss: 0.062776, Train Acc: 0.980326, Valid Loss: 0.102514, Valid Acc: 0.977576, Time 00:05:10\n",
      "----------\n",
      "Epoch 0. Train Loss: 1.267297, Train Acc: 0.566779, Valid Loss: 0.920528, Valid Acc: 0.695611, Time 00:04:52\n",
      "Epoch 1. Train Loss: 0.748388, Train Acc: 0.751200, Valid Loss: 0.449545, Valid Acc: 0.855439, Time 00:05:08\n",
      "Epoch 2. Train Loss: 0.538597, Train Acc: 0.821417, Valid Loss: 0.466843, Valid Acc: 0.853531, Time 00:05:09\n",
      "Epoch 3. Train Loss: 0.433861, Train Acc: 0.855966, Valid Loss: 0.340359, Valid Acc: 0.899332, Time 00:05:10\n",
      "Epoch 4. Train Loss: 0.349355, Train Acc: 0.885677, Valid Loss: 0.278049, Valid Acc: 0.909828, Time 00:05:09\n",
      "Epoch 5. Train Loss: 0.316404, Train Acc: 0.894234, Valid Loss: 0.209014, Valid Acc: 0.937023, Time 00:05:09\n",
      "Epoch 6. Train Loss: 0.268945, Train Acc: 0.912068, Valid Loss: 0.215335, Valid Acc: 0.927481, Time 00:05:09\n",
      "Epoch 7. Train Loss: 0.244327, Train Acc: 0.919506, Valid Loss: 0.228357, Valid Acc: 0.929389, Time 00:05:09\n",
      "Epoch 8. Train Loss: 0.203799, Train Acc: 0.933461, Valid Loss: 0.176498, Valid Acc: 0.951813, Time 00:05:09\n",
      "Epoch 9. Train Loss: 0.197379, Train Acc: 0.933341, Valid Loss: 0.157211, Valid Acc: 0.952767, Time 00:05:09\n",
      "Epoch 10. Train Loss: 0.172723, Train Acc: 0.945577, Valid Loss: 0.101494, Valid Acc: 0.968034, Time 00:05:09\n",
      "Epoch 11. Train Loss: 0.101738, Train Acc: 0.966811, Valid Loss: 0.080650, Valid Acc: 0.972805, Time 00:05:09\n",
      "Epoch 12. Train Loss: 0.082363, Train Acc: 0.975168, Valid Loss: 0.084073, Valid Acc: 0.974237, Time 00:05:08\n",
      "Epoch 13. Train Loss: 0.074239, Train Acc: 0.975528, Valid Loss: 0.079411, Valid Acc: 0.974714, Time 00:05:09\n",
      "Epoch 14. Train Loss: 0.074437, Train Acc: 0.977807, Valid Loss: 0.079112, Valid Acc: 0.975191, Time 00:05:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15. Train Loss: 0.066457, Train Acc: 0.978527, Valid Loss: 0.078544, Valid Acc: 0.974714, Time 00:05:08\n",
      "Epoch 16. Train Loss: 0.063151, Train Acc: 0.978567, Valid Loss: 0.091413, Valid Acc: 0.970420, Time 00:05:10\n",
      "Epoch 17. Train Loss: 0.061567, Train Acc: 0.980326, Valid Loss: 0.073479, Valid Acc: 0.976622, Time 00:05:08\n",
      "Epoch 18. Train Loss: 0.061634, Train Acc: 0.980686, Valid Loss: 0.077448, Valid Acc: 0.976145, Time 00:05:09\n",
      "Epoch 19. Train Loss: 0.059609, Train Acc: 0.979966, Valid Loss: 0.077585, Valid Acc: 0.974714, Time 00:05:08\n",
      "----------\n",
      "Epoch 0. Train Loss: 1.315196, Train Acc: 0.552863, Valid Loss: 0.819364, Valid Acc: 0.732347, Time 00:04:53\n",
      "Epoch 1. Train Loss: 0.757359, Train Acc: 0.751040, Valid Loss: 0.646199, Valid Acc: 0.801050, Time 00:05:10\n",
      "Epoch 2. Train Loss: 0.564984, Train Acc: 0.812500, Valid Loss: 0.347704, Valid Acc: 0.880248, Time 00:05:09\n",
      "Epoch 3. Train Loss: 0.441108, Train Acc: 0.853807, Valid Loss: 0.286077, Valid Acc: 0.906011, Time 00:05:10\n",
      "Epoch 4. Train Loss: 0.352723, Train Acc: 0.885237, Valid Loss: 0.351191, Valid Acc: 0.894084, Time 00:05:09\n",
      "Epoch 5. Train Loss: 0.295124, Train Acc: 0.902271, Valid Loss: 0.211765, Valid Acc: 0.937023, Time 00:05:08\n",
      "Epoch 6. Train Loss: 0.270486, Train Acc: 0.911708, Valid Loss: 0.192337, Valid Acc: 0.944656, Time 00:05:09\n",
      "Epoch 7. Train Loss: 0.241298, Train Acc: 0.921545, Valid Loss: 0.209311, Valid Acc: 0.936069, Time 00:05:09\n",
      "Epoch 8. Train Loss: 0.200089, Train Acc: 0.934861, Valid Loss: 0.189825, Valid Acc: 0.954198, Time 00:05:08\n",
      "Epoch 9. Train Loss: 0.180527, Train Acc: 0.943258, Valid Loss: 0.176330, Valid Acc: 0.950382, Time 00:05:09\n",
      "Epoch 10. Train Loss: 0.168427, Train Acc: 0.945937, Valid Loss: 0.197905, Valid Acc: 0.952290, Time 00:05:10\n",
      "Epoch 11. Train Loss: 0.114010, Train Acc: 0.962252, Valid Loss: 0.141243, Valid Acc: 0.969466, Time 00:05:08\n",
      "Epoch 12. Train Loss: 0.090520, Train Acc: 0.968450, Valid Loss: 0.136970, Valid Acc: 0.968034, Time 00:05:09\n",
      "Epoch 13. Train Loss: 0.077503, Train Acc: 0.974128, Valid Loss: 0.130296, Valid Acc: 0.970897, Time 00:05:09\n",
      "Epoch 14. Train Loss: 0.070053, Train Acc: 0.975688, Valid Loss: 0.136858, Valid Acc: 0.970420, Time 00:05:08\n",
      "Epoch 15. Train Loss: 0.072658, Train Acc: 0.974728, Valid Loss: 0.132118, Valid Acc: 0.971374, Time 00:05:08\n",
      "Epoch 16. Train Loss: 0.058324, Train Acc: 0.981166, Valid Loss: 0.133518, Valid Acc: 0.972328, Time 00:05:09\n",
      "Epoch 17. Train Loss: 0.064479, Train Acc: 0.980246, Valid Loss: 0.132915, Valid Acc: 0.972328, Time 00:05:09\n",
      "Epoch 18. Train Loss: 0.060563, Train Acc: 0.979367, Valid Loss: 0.131010, Valid Acc: 0.971374, Time 00:05:08\n",
      "Epoch 19. Train Loss: 0.056837, Train Acc: 0.981406, Valid Loss: 0.128219, Valid Acc: 0.971374, Time 00:05:10\n",
      "----------\n",
      "   models  best_accury\n",
      "0  model0     0.976622\n",
      "1  model1     0.970897\n",
      "2  model2     0.978531\n",
      "3  model3     0.976622\n",
      "4  model4     0.972328\n"
     ]
    }
   ],
   "source": [
    "# resnet50\n",
    "best_accury = []\n",
    "Models = []\n",
    "model_names = []\n",
    "for i in range(len(train_loader)):\n",
    "    model_names.append('model'+str(i))\n",
    "    Models.append('model'+str(i))\n",
    "for i in range(len(train_loader)):\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    model.fc = nn.Linear(model.fc.in_features,10)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[11,16], gamma=0.1)\n",
    "    \n",
    "    model = model.to('mps')\n",
    "    \n",
    "    best_acc, Models[i] = train(model,train_loader[i],test_loader[i],20,optimizer,criterion,scheduler)\n",
    "    best_accury.append(best_acc)\n",
    "    \n",
    "    print('-'*10)\n",
    "    \n",
    "print(pd.DataFrame().assign(models=model_names,best_accury=best_accury)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e825251",
   "metadata": {},
   "source": [
    "convnext_tiny  model.classifier[2] = nn.Linear(model.classifier[2].in_features,10)\n",
    "vit_l_16  model.heads.head = nn.Linear(model.heads.head.in_features,10)\n",
    "mobilenet_v3_small  model.classifier[3] = nn.Linear(model.classifier[3].in_features,10)\n",
    "VGG16  model.classifier[6] = nn.Linear(model.classifier[6].in_features,10)\n",
    "mobilenet_v2  model.classifier[1] = nn.Linear(model.classifier[1].in_features,10) \n",
    "Inception v3  model.AuxLogits.fc = nn.Linear(768, 10)  model.fc = nn.Linear(2048, 10)\n",
    "googlenet  model.fc = nn.Linear(model.fc.in_features,10)\n",
    "efficientnet_b0  model.classifier[1] = nn.Linear(model.classifier[1].in_features,10ï¼‰\n",
    "resnet50    model.fc = nn.Linear(model.fc.in_features,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3fea9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50',pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2501f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('resnet50',pretrained=True)\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "model.fc = nn.Linear(model.fc.in_features,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e5f04703",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Net \n",
    "class Head(nn.Module):\n",
    "    def __init__(self,in_features,out_features):\n",
    "        super(Head,self).__init__()\n",
    "        self.head = nn.Linear(in_features=in_features,out_features=out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.head(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model = 'resnet50',\n",
    "        pretrained = True,\n",
    "        checkpoint_path = None,\n",
    "        num_classes = 10\n",
    "    ):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.backbone = timm.create_model(base_model,pretrained=pretrained,checkpoint_path=checkpoint_path)\n",
    "        in_features = self.backbone.get_classifier().in_features\n",
    "        self.backbone.reset_classifier(num_classes=0,global_pool='avg')\n",
    "        self.neck = Head(in_features=in_features, out_features=in_features)\n",
    "        self.head = Head(in_features=in_features, out_features=num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.neck(x)\n",
    "        x = self.head(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6824b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('efficientnet_b3',pretrained=True)\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a8133efb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def str_cro():\n",
    "    best_accury = []\n",
    "    Models = []\n",
    "    model_names = []\n",
    "    \n",
    "    for i in range(len(train_loader)):\n",
    "        \n",
    "        model_names.append('model'+str(i))\n",
    "        Models.append('model'+str(i))\n",
    "        \n",
    "    for i in range(len(train_loader)):  \n",
    "        \n",
    "        model = timm.create_model('efficientnet_b3',pretrained=True)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.classifier = nn.Linear(model.classifier.in_features,10)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[11,16], gamma=0.1)\n",
    "        \n",
    "        model = model.to('mps:0')\n",
    "\n",
    "\n",
    "        best_acc, Models[i] = train_new(model,train_loader[i],test_loader[i],20,optimizer,criterion,scheduler)\n",
    "        best_accury.append(best_acc)\n",
    "\n",
    "        print('-'*10)\n",
    "\n",
    "    print(pd.DataFrame().assign(models=model_names,best_accury=best_accury))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ece3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cro() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "40a9879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930047d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for im, label in train_loader[0]:\n",
    "\n",
    "    im = im.to('mps:0')\n",
    "    label = label.to('mps:0')\n",
    "    # zero  gradients\n",
    "    optimizer.zero_grad()\n",
    "    # forward \n",
    "    output = model(im)  \n",
    "    \n",
    "    print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9e0035c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01,momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "73c3c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('convnext_tiny',pretrained=True)\n",
    "\n",
    "model.head.fc =  nn.Linear(model.head.fc.in_features,10)\n",
    "\n",
    "model = model.to('mps:0')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58cae01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(Models[2].state_dict(),'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "637e8d31",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = models.resnet50()\n",
    "model.fc = nn.Linear(model.fc.in_features,10)\n",
    "model.load_state_dict(torch.load('resnet50'))\n",
    "model = model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec34a8b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "model.eval()\n",
    "image_id = []\n",
    "label = []\n",
    "test_dir = path/'Test_images'\n",
    "\n",
    "for img in (test_dir.iterdir()):\n",
    "    img_id = img.name\n",
    "    img = Test_transform(Image.open(img)).unsqueeze(0).to('mps')\n",
    "    cat = train_data.dataset.dataset.classes[model(img).argmax().item()]\n",
    "    image_id.append(img_id)\n",
    "    label.append(cat)\n",
    "\n",
    "result = pd.DataFrame().assign(image_id=image_id,label=label)\n",
    "result.to_csv('result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66788f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'The \"portfolio\" repository contains a collection of my personal skills and projects, showcasing my experience and abilities in computer vision, machine learning, and deep learning. It includes recent and past projects that demonstrate my coding capabilities and engineering practice experience. All the projects here are completed independently by me, with detailed documentation. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3c4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ee0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
